# 04 - Design for Failure and Resilience

> üí° *"In distributed systems, failure is not a bug ‚Äî it's a feature of reality."*

This principle is the **mindset shift** that separates junior engineers from senior system designers.

Microservices run across networks, containers, databases, and cloud providers. At scale, **something is always failing** ‚Äî a service crashes, the network slows, a database times out.

If your system assumes "everything works", it will fail **catastrophically**.

But if you **design for failure**, your system becomes **resilient** ‚Äî it survives, degrades gracefully, and recovers.

Let's go deep.

---

## üí• What Is "Catastrophic Failure"?

> ‚ùå **Catastrophic failure** = A small problem triggers a **chain reaction** that brings down the **entire system**, even parts that could have kept working.

It's not just "a service is down."  
It's:  
‚û°Ô∏è One service fails ‚Üí  
‚û°Ô∏è Others wait for it ‚Üí  
‚û°Ô∏è Their threads fill up ‚Üí  
‚û°Ô∏è They start failing ‚Üí  
‚û°Ô∏è The **whole app becomes unresponsive** ‚Äî  
‚û°Ô∏è Even for features that had **nothing to do** with the original failure.

This is called **cascading failure**.

---

### üî• Real-World Example: E-Commerce on Black Friday

```text
User ‚Üí Clicks "Place Order"
       ‚Üì
Order Service ‚Üí Calls ‚Üí Payment Service
                         ‚Üë
                  Payment Service is slow (due to bug)
                         ‚Üë
Order Service waits... and waits... (no timeout!)
                         ‚Üë
10,000 users placing orders ‚Üí 10,000 pending requests
                         ‚Üë
Order Service runs out of threads ‚Üí becomes unresponsive
                         ‚Üë
Now even "View Cart" fails ‚Äî even though it doesn't need Payment Service!
```

üéØ Result:  
‚ùå The **entire site is down**  
‚ùå Because of **one slow service**  
‚ùå And **no resilience** (no timeout, no fallback)

That's **catastrophic failure**.

---

## üß± Why This Happens

In microservices, services talk over the **network** ‚Äî which is **unreliable by nature**.

| Wrong Assumption | Reality |
|------------------|--------|
| "The network is reliable" | ‚ùå Packets get lost, delayed, dropped |
| "Services always respond fast" | ‚ùå Services can be slow or crash |
| "One failure only affects one feature" | ‚ùå Without safeguards, it spreads |

When you **don't design for failure**, you're building a house on sand.

---

## ‚úÖ The 4 Pillars of Resilience

| Pattern | Purpose |
|--------|--------|
| **Timeouts** | Don't wait forever |
| **Retries (with backoff)** | Try again, but wisely |
| **Circuit Breaker** | Stop hammering a dead service |
| **Fallbacks** | Keep working with degraded functionality |

Let's explore each.

---

## 1Ô∏è‚É£ Timeouts: Don't Wait Forever

### ‚ùå Problem:
```js
const res = await fetch('http://payment-service/process'); // Waits... forever?
```
If `payment-service` hangs, your service also hangs ‚Üí **cascading failure**.

### ‚úÖ Solution: Set a timeout
```js
const controller = new AbortController();
const timeoutId = setTimeout(() => controller.abort(), 5000); // 5 sec

try {
  const res = await fetch('http://payment-service/process', {
    method: 'POST',
    signal: controller.signal
  });
} catch (err) {
  console.log('Payment service slow or down.');
  return res.status(503).send('Payment temporarily unavailable');
}
```

> ‚úÖ **Rule**: Every external call must have a timeout.

---

## 2Ô∏è‚É£ Retries with Exponential Backoff

Sometimes a failure is temporary (network glitch). A **smart retry** can fix it.

### ‚úÖ Example: Retry with Backoff
```js
async function callWithRetry(url, maxRetries = 3) {
  let lastError;
  for (let i = 0; i <= maxRetries; i++) {
    try {
      const res = await fetch(url, { signal: timeoutSignal(5000) });
      if (res.ok) return await res.json();
    } catch (err) {
      lastError = err;
      if (i < maxRetries) {
        const delay = Math.pow(2, i) * 1000; // 1s, 2s, 4s
        await new Promise(r => setTimeout(r, delay));
        console.log(`Retry ${i + 1} after ${delay}ms`);
      }
    }
  }
  throw lastError;
}
```

#### ‚ö†Ô∏è But Beware:
- Don't retry on `400 Bad Request` (user error)
- Do retry on `503 Service Unavailable`
- Use **exponential backoff** to avoid overwhelming the system

---

## 3Ô∏è‚É£ Circuit Breaker: Stop Hammering a Dead Service

### ‚ùå Problem:
If `Payment Service` is down, 1000 requests per second keep failing ‚Üí logs fill up, threads block.

### ‚úÖ Solution: Circuit Breaker Pattern

Like a real circuit breaker in your house:
- One short circuit ‚Üí breaker trips
- No more power ‚Üí prevents fire
- After timeout, tries to restore

Same idea.

### üîÑ Three States:

```mermaid
graph LR
  A[Closed] -->|"Failures >= threshold"| B[Open]
  B -->|"After resetTimeout"| C[Half-Open]
  C -->|Success| A
  C -->|Failure| B
```

- **Closed**: Requests go through
- **Open**: All requests fail immediately
- **Half-Open**: Let 1 request through to test recovery

---

### üí° Using Opossum (Node.js Circuit Breaker)

```bash
npm install opossum
```

```js
const circuitBreaker = require('opossum');
const fetch = require('node-fetch');

async function callPaymentService(orderId) {
  const res = await fetch(`http://localhost:4000/process/${orderId}`);
  if (!res.ok) throw new Error('Payment failed');
  return res.json();
}

const options = {
  timeout: 3000,
  errorThresholdPercentage: 50,
  resetTimeout: 10000
};

const breaker = new circuitBreaker(callPaymentService, options);

// Fallback
breaker.fallback(() => ({ status: 'failed', reason: 'Service down. Try later.' }));

// Events
breaker.on('open', () => console.log('üî¥ Circuit opened'));
breaker.on('halfOpen', () => console.log('üîß Testing recovery'));
breaker.on('close', () => console.log('‚úÖ Circuit closed'));

// Use it
app.post('/order', async (req, res) => {
  try {
    const result = await breaker.fire(req.body.orderId);
    res.json({ status: 'success', data: result });
  } catch (err) {
    res.status(503).json({ error: err.message });
  }
});
```

---

## 4Ô∏è‚É£ Fallbacks: Have a Backup Plan

When a service fails, can you still provide **degraded functionality**?

### ‚úÖ Example: Return Cached Data
```js
async function getUserWithFallback(userId) {
  try {
    const res = await fetchWithTimeout(`http://user-service/users/${userId}`, 3000);
    if (res.ok) return await res.json();
  } catch (err) {
    console.log('User service failed, using cache');
    const cached = await getFromRedis(`user:${userId}`);
    if (cached) return JSON.parse(cached);
    return { id: userId, name: 'Unknown (service down)' };
  }
}
```

#### Other Fallback Ideas:
- Show stale data
- Skip non-critical steps (e.g., don't send email now)
- Use a simpler algorithm

> ‚úÖ Goal: **Keep the user experience alive**, even if limited.

---

## üõ†Ô∏è Tools That Help

| Tool | Purpose |
|------|--------|
| **Opossum** | Circuit breaker for Node.js |
| **Redis** | Cache fallback data |
| **Kafka/SQS** | Queue failed work for later |
| **Sentry/New Relic** | Monitor failures |
| **Gremlin/Chaos Monkey** | Test resilience by injecting failures |

---

## üß© Resilience in Practice: Order Flow Example

```text
User places order
  ‚Üì
Call Payment Service (with timeout: 5s)
  ‚Üì
If timeout ‚Üí Retry (3x, exponential backoff)
  ‚Üì
If still fails ‚Üí Open circuit breaker
  ‚Üì
Return: "Order saved, payment pending"
  ‚Üì
Publish: OrderPaymentFailed ‚Üí retry later via job
  ‚Üì
User can retry payment later
```

‚úÖ System didn't crash.  
‚úÖ User wasn't blocked.  
‚úÖ Core function (create order) succeeded.

That's **resilience**.

---

## üö´ Anti-Patterns to Avoid

| Mistake | Why Bad |
|--------|--------|
| No timeout | One slow service blocks all others |
| Unlimited retries | DDoS your own system |
| No circuit breaker | Cascading failures |
| No fallback | Entire feature breaks on one failure |
| Silent failures | Can't debug or alert |

---

## üìä Diagram: Resilience Patterns in Action

```mermaid
graph TD
  A[Client] --> B[Order Service]
  B --> C{Call Payment Service}
  C -->|Success| D[Order Confirmed]
  C -->|Timeout| E[Retry with Backoff]
  E -->|"Still Fails"| F[Circuit Breaker Opens]
  F --> G["Fallback: Payment Pending"]
  G --> H[Save Order]
  H --> I[Publish: Retry Later]
  I --> J[Kafka Queue]
  J --> K[Background Job]
  K --> C
```

‚úÖ Even when `Payment Service` is down, the system **keeps working**.

---

## üß† What Is Chaos Engineering?

> **Chaos Engineering** = *Intentionally breaking your system in production-like environments to learn how it fails ‚Äî so you can fix it before it matters.*

It's like **fire drills** for software.

### üî¨ Examples:
- Kill a service instance
- Inject 2s network delay
- Fill up disk space
- Cut off database

### ‚úÖ Why It Matters:
- Finds hidden dependencies
- Tests your monitoring
- Builds team confidence

> üí° Rule: **Test in staging first. Never surprise users.**

---

## üî§ What Is Idempotency?

> **Idempotent** = Calling the same operation multiple times has the **same effect** as calling it once.

### ‚úÖ Example:
```http
DELETE /orders/123 ‚Üí idempotent
PUT /users/123 ‚Üí idempotent
```

```http
POST /orders ‚Üí not idempotent
```

### ‚úÖ Solution: Use Idempotency Keys
```http
POST /orders
Idempotency-Key: order_abc123
```

Server checks if key was used ‚Üí if yes, returns same result ‚Üí prevents duplicates.

> üí° Critical for payments, orders, retries.

---

## üéØ Summary: Resilience Checklist

| Do | Example |
|----|--------|
| ‚úÖ Always use timeouts | `fetch(url, { signal: AbortController with 5s })` |
| ‚úÖ Retry with backoff | 1s, 2s, 4s ‚Äî not instantly |
| ‚úÖ Use circuit breakers | Stop calling a failing service |
| ‚úÖ Provide fallbacks | Cache, default values, async recovery |
| ‚úÖ Monitor failures | Log, alert, trace |
| ‚úÖ Test failure modes | Chaos engineering |

---

## üìö References & Further Reading

- [Martin Fowler: Circuit Breaker](https://martinfowler.com/bliki/CircuitBreaker.html)
- [Netflix Chaos Engineering](https://netflixtechblog.com/chaos-engineering-4c84b07efc56)
- [Opossum Docs](https://github.com/nodeshift/opossum)
- [Building Microservices by Sam Newman](https://www.oreilly.com/library/view/building-microservices/9781491931879/)
- [Google SRE Book](https://sre.google/books/)

> üßì Mentor's Note:  
> You can't prevent all failures.  
> But you can **design for them** ‚Äî contain them, recover from them, and keep the system alive.  
> That's the mark of a true systems engineer.